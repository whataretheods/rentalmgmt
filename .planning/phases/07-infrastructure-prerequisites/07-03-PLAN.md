---
phase: 07-infrastructure-prerequisites
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - src/lib/storage.ts
  - src/lib/uploads.ts
  - src/app/api/maintenance/route.ts
  - src/app/api/documents/route.ts
  - src/app/api/uploads/[...path]/route.ts
  - src/app/api/documents/[id]/route.ts
autonomous: true
requirements: [INFRA-01]
user_setup:
  - service: "Cloudflare R2 (or any S3-compatible storage)"
    why: "Cloud file storage for maintenance photos and documents"
    env_vars:
      - name: S3_ENDPOINT
        source: "R2 Dashboard -> Overview -> S3 API endpoint (https://<account_id>.r2.cloudflarestorage.com)"
      - name: S3_ACCESS_KEY_ID
        source: "R2 Dashboard -> Manage R2 API Tokens -> Create API Token -> Access Key ID"
      - name: S3_SECRET_ACCESS_KEY
        source: "R2 Dashboard -> Manage R2 API Tokens -> Create API Token -> Secret Access Key"
      - name: S3_BUCKET_NAME
        source: "R2 Dashboard -> Create Bucket (e.g., 'rentalmgmt-uploads')"
    dashboard_config:
      - task: "Create R2 bucket"
        location: "Cloudflare Dashboard -> R2 Object Storage -> Create Bucket"
      - task: "Create API token with read/write permissions"
        location: "Cloudflare Dashboard -> R2 -> Manage R2 API Tokens"
      - task: "Configure CORS (allow GET from app domain)"
        location: "R2 Bucket -> Settings -> CORS Policy"

must_haves:
  truths:
    - "New file uploads (maintenance photos, documents) are stored in S3-compatible cloud storage instead of the local filesystem"
    - "Existing local files continue to be served correctly via the dual-read pattern (storageBackend column)"
    - "Downloaded files use presigned URLs for S3 storage, redirecting the browser directly to R2/S3"
    - "Upload validation (file size, MIME type) still enforced before S3 upload"
  artifacts:
    - path: "src/lib/storage.ts"
      provides: "S3 client initialization, uploadToS3, getPresignedDownloadUrl helpers"
      exports: ["uploadToS3", "getPresignedDownloadUrl"]
    - path: "src/lib/uploads.ts"
      provides: "Updated saveUploadedFile that stores to S3 and returns s3Key + storageBackend"
      exports: ["saveUploadedFile", "UPLOADS_DIR", "ALLOWED_MIME_TYPES", "MAX_FILE_SIZE"]
    - path: "src/app/api/uploads/[...path]/route.ts"
      provides: "Dual-read file serving (S3 presigned URL or local file)"
    - path: "src/app/api/documents/[id]/route.ts"
      provides: "Dual-read document serving with storageBackend check"
  key_links:
    - from: "src/lib/uploads.ts"
      to: "src/lib/storage.ts"
      via: "import uploadToS3"
      pattern: "import.*uploadToS3.*from.*storage"
    - from: "src/app/api/uploads/[...path]/route.ts"
      to: "src/lib/storage.ts"
      via: "import getPresignedDownloadUrl for S3 files"
      pattern: "getPresignedDownloadUrl"
    - from: "src/app/api/maintenance/route.ts"
      to: "src/db/schema/domain.ts"
      via: "inserts storageBackend and s3Key columns"
      pattern: "storageBackend.*s3Key"
---

<objective>
Implement S3-compatible cloud storage for file uploads with a dual-read pattern that serves existing local files transparently.

Purpose: Files are currently stored on the local filesystem (`uploads/` directory), which does not survive redeployments in serverless/container environments. New uploads go to S3/R2 with presigned URLs for download. Existing local files continue to work via the `storageBackend` column.

Output: S3 client library, updated upload function, dual-read serving routes, all wired to the schema columns from Plan 02.
</objective>

<execution_context>
@/Users/odesantos/.claude/get-shit-done/workflows/execute-plan.md
@/Users/odesantos/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/07-infrastructure-prerequisites/07-RESEARCH.md
@.planning/phases/07-infrastructure-prerequisites/07-02-SUMMARY.md

<interfaces>
<!-- Schema columns available after Plan 02 migration -->
From src/db/schema/domain.ts (maintenancePhotos table):
```typescript
storageBackend: text("storage_backend").default("local").notNull(),
s3Key: text("s3_key"),
```

From src/db/schema/domain.ts (documents table):
```typescript
storageBackend: text("storage_backend").default("local").notNull(),
s3Key: text("s3_key"),
```

<!-- Current upload function signature to maintain backward compatibility -->
From src/lib/uploads.ts:
```typescript
export const UPLOADS_DIR = path.join(process.cwd(), "uploads")
export const ALLOWED_MIME_TYPES = new Set([...])
export const MAX_FILE_SIZE = 25 * 1024 * 1024

export async function saveUploadedFile(
  file: File,
  subdirectory: string,
): Promise<{ filePath: string; fileName: string; fileSize: number; mimeType: string }>
```

<!-- Current upload consumers -->
From src/app/api/maintenance/route.ts:
```typescript
import { saveUploadedFile } from "@/lib/uploads"
// uses: const uploaded = await saveUploadedFile(file, "maintenance")
// then inserts: filePath: uploaded.filePath, fileName: uploaded.fileName, ...
```

From src/app/api/documents/route.ts:
```typescript
import { saveUploadedFile } from "@/lib/uploads"
// uses: uploadResult = await saveUploadedFile(file, "documents")
// then inserts: filePath: uploadResult.filePath, ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create S3 client library and update upload function</name>
  <files>src/lib/storage.ts, src/lib/uploads.ts, package.json</files>
  <action>
1. Install AWS SDK S3 packages:
   ```bash
   npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner
   ```

2. Create `src/lib/storage.ts` — the S3 client and helper functions:

   ```typescript
   import { S3Client, PutObjectCommand, GetObjectCommand } from "@aws-sdk/client-s3"
   import { getSignedUrl } from "@aws-sdk/s3-request-presigner"
   ```

   **S3 client:** Create a lazy-initialized singleton `S3Client` (same pattern as db — don't create at import time). Configuration:
   - `region: "auto"` (R2 uses "auto"; S3 would use specific region)
   - `endpoint: process.env.S3_ENDPOINT` (R2: `https://<account_id>.r2.cloudflarestorage.com`)
   - `credentials` from `S3_ACCESS_KEY_ID` and `S3_SECRET_ACCESS_KEY`

   **`uploadToS3(buffer: Buffer, key: string, contentType: string): Promise<string>`**
   - Sends `PutObjectCommand` with Bucket from `S3_BUCKET_NAME` env var
   - Returns the key (not a URL)

   **`getPresignedDownloadUrl(key: string): Promise<string>`**
   - Generates a presigned GET URL with 1-hour expiry (`expiresIn: 3600`)
   - Returns the signed URL string

   **`isS3Configured(): boolean`**
   - Returns true if all 4 S3 env vars are set (S3_ENDPOINT, S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY, S3_BUCKET_NAME)
   - Used as a graceful fallback — if S3 is not configured, uploads fall back to local storage

3. Update `src/lib/uploads.ts` — change `saveUploadedFile` to upload to S3:

   The updated function must:
   a. Keep the SAME validation logic (MIME type check, file size check)
   b. Check `isS3Configured()` from storage.ts
   c. If S3 is configured: upload to S3 using `uploadToS3`, return `{ filePath: "", s3Key: key, fileName, fileSize, mimeType, storageBackend: "s3" }`
   d. If S3 is NOT configured: fall back to existing local file save logic, return `{ filePath: relPath, s3Key: null, fileName, fileSize, mimeType, storageBackend: "local" }`

   **Return type changes:** The return type must be expanded to include `s3Key: string | null` and `storageBackend: "local" | "s3"`. The `filePath` field remains for backward compatibility (empty string for S3, relative path for local).

   Keep the existing exports (`UPLOADS_DIR`, `ALLOWED_MIME_TYPES`, `MAX_FILE_SIZE`) — they're used by file-serving routes.
  </action>
  <verify>
    <automated>cd /Users/odesantos/Documents/rentalmgmt && test -f src/lib/storage.ts && grep -q "uploadToS3" src/lib/storage.ts && grep -q "getPresignedDownloadUrl" src/lib/storage.ts && grep -q "isS3Configured" src/lib/storage.ts && grep -q "storageBackend" src/lib/uploads.ts && echo "PASS: storage.ts and uploads.ts updated" || echo "FAIL: missing expected functions"</automated>
  </verify>
  <done>src/lib/storage.ts created with S3 client, uploadToS3, getPresignedDownloadUrl, and isS3Configured. src/lib/uploads.ts updated to upload to S3 when configured, with graceful local fallback. Return type includes storageBackend and s3Key.</done>
</task>

<task type="auto">
  <name>Task 2: Update API routes for S3 upload and dual-read serving</name>
  <files>src/app/api/maintenance/route.ts, src/app/api/documents/route.ts, src/app/api/uploads/[...path]/route.ts, src/app/api/documents/[id]/route.ts</files>
  <action>
1. **Update `src/app/api/maintenance/route.ts` POST handler:**

   After calling `saveUploadedFile`, the returned object now includes `s3Key` and `storageBackend`. Update the `maintenancePhotos` insert to include these new columns:
   ```typescript
   const [photo] = await db.insert(maintenancePhotos).values({
     requestId: request.id,
     filePath: uploaded.filePath,
     fileName: uploaded.fileName,
     fileSize: uploaded.fileSize,
     mimeType: uploaded.mimeType,
     storageBackend: uploaded.storageBackend,
     s3Key: uploaded.s3Key,
   }).returning()
   ```

2. **Update `src/app/api/documents/route.ts` POST handler:**

   Same pattern — update the `documents` insert to include `storageBackend` and `s3Key`:
   ```typescript
   const [doc] = await db.insert(documents).values({
     tenantUserId: session.user.id,
     documentType: ...,
     filePath: uploadResult.filePath,
     fileName: uploadResult.fileName,
     fileSize: uploadResult.fileSize,
     mimeType: uploadResult.mimeType,
     storageBackend: uploadResult.storageBackend,
     s3Key: uploadResult.s3Key,
     requestId: requestId || null,
   }).returning()
   ```

3. **Update `src/app/api/uploads/[...path]/route.ts` GET handler (maintenance photo serving):**

   This route currently serves files directly from the local filesystem. Add dual-read logic:
   - Import `getPresignedDownloadUrl` from `@/lib/storage`
   - Import `db` and `maintenancePhotos` from schema
   - The current route uses path segments to find files. For S3 files, we need to check the database for storageBackend.
   - Since this route serves maintenance photos by file path, look up the photo record by `filePath` matching the path segments.
   - If `storageBackend === "s3"` and `s3Key` is set: redirect to `getPresignedDownloadUrl(s3Key)`
   - If `storageBackend === "local"` or no match found: serve from local filesystem (existing logic)

   **IMPORTANT:** The presigned URL redirect should use `NextResponse.redirect(url)` with a 302 status. This sends the browser directly to R2/S3 — the file bytes never pass through the app server.

4. **Update `src/app/api/documents/[id]/route.ts` GET handler (document serving):**

   This route already looks up the document by ID from the database. Add dual-read:
   - Import `getPresignedDownloadUrl` from `@/lib/storage`
   - After fetching the document record, check `doc.storageBackend`
   - If `"s3"` and `doc.s3Key`: redirect to presigned URL
   - If `"local"`: serve from local filesystem (existing behavior)

5. Run the full E2E suite to verify no regressions:
   ```bash
   npx playwright test
   ```
  </action>
  <verify>
    <automated>cd /Users/odesantos/Documents/rentalmgmt && grep -q "storageBackend" src/app/api/maintenance/route.ts && grep -q "storageBackend" src/app/api/documents/route.ts && grep -q "getPresignedDownloadUrl" src/app/api/uploads/\[...path\]/route.ts && grep -q "getPresignedDownloadUrl" src/app/api/documents/\[id\]/route.ts && echo "PASS: all routes updated" || echo "FAIL: routes missing S3 integration"</automated>
  </verify>
  <done>All 4 API routes updated: maintenance and document POST routes write storageBackend + s3Key to database. Upload serving route and document serving route implement dual-read (S3 presigned redirect or local file). Existing local files continue to work via fallback.</done>
</task>

</tasks>

<verification>
1. `src/lib/storage.ts` exports uploadToS3, getPresignedDownloadUrl, isS3Configured
2. `src/lib/uploads.ts` returns storageBackend and s3Key in upload result
3. All 4 API routes integrate storageBackend/s3Key
4. Without S3 env vars configured, uploads still work via local fallback (graceful degradation)
5. `npx playwright test` passes (existing tests still work with local fallback)
</verification>

<success_criteria>
- New uploads go to S3 when S3_ENDPOINT/S3_ACCESS_KEY_ID/S3_SECRET_ACCESS_KEY/S3_BUCKET_NAME are configured
- Existing local files continue to be served via the dual-read pattern
- Presigned download URLs redirect browser directly to R2/S3 (no proxying through app server)
- Without S3 config, the application gracefully falls back to local file storage (development-friendly)
- No regressions in existing maintenance photo or document upload flows
</success_criteria>

<output>
After completion, create `.planning/phases/07-infrastructure-prerequisites/07-03-SUMMARY.md`
</output>
